# LLM Compression Benchmarks
[x] Datasets commonly used in LLM compression/quantization/pruning papers.

Includes C4/RedPajama for pretrain-scale evaluation, WikiText-2/3 and PTB for perplexity,
and calibration splits for post-training quantization (SmoothQuant, AWQ-style), plus small
KD subsets for low-data distillation experiments.
